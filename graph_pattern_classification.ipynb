{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uvA7PvefrbFV",
    "outputId": "7ecb31b5-5141-4379-b715-33016b89a9e1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "from typing import Callable, List, Optional\n",
    "from torch_geometric.data import Data, InMemoryDataset, download_url\n",
    "import os.path as osp\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PyG Dataset\n",
    "\n",
    "Create a dataset class."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class GPClassification(InMemoryDataset):\n",
    "  def __init__(self, root: str, dataset_name: str = \"GPClassificationTest_01\",\n",
    "              transform: Optional[Callable] = None,\n",
    "              pre_transform: Optional[Callable] = None):\n",
    "\n",
    "    self.dataset_name = dataset_name\n",
    "    super().__init__(root, transform, pre_transform)\n",
    "\n",
    "    self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "    node_dict = torch.load(self.processed_paths[1])\n",
    "    rel_dict = torch.load(self.processed_paths[2])\n",
    "\n",
    "    self.num_relations=len(rel_dict.keys())\n",
    "\n",
    "\n",
    "  @property\n",
    "  def raw_file_names(self) -> List[str]:\n",
    "      return ['graphs.json', 'targets.csv']\n",
    "\n",
    "  @property\n",
    "  def processed_file_names(self) -> List[str]:\n",
    "      return ['graphs.pt', 'node_dict.pt', 'rel_dict.pt']\n",
    "\n",
    "\n",
    "  def download(self):\n",
    "    print('Please place the required files in the raw directory')\n",
    "\n",
    "  def process(self):\n",
    "    data_list, node_dict, rel_dict = [], {}, {}\n",
    "\n",
    "    graph_dict = json.load(open(osp.join(self.raw_dir, 'graphs.json'), 'r'))\n",
    "\n",
    "    targets = pd.read_csv(osp.join(self.raw_dir, 'targets.csv'), header=None,  names=['id', 'label'])\n",
    "\n",
    "    self.num_nodes = max(targets['id'])\n",
    "    x = torch.range(0, self.num_nodes + 1)\n",
    "\n",
    "\n",
    "    for graph_id, graph in graph_dict.items():\n",
    "      edge_index = []\n",
    "      edge_type = []\n",
    "      for head, relation, tail in graph:\n",
    "        if head not in node_dict:\n",
    "          node_dict[head] = len(node_dict.keys())\n",
    "        if tail not in node_dict:\n",
    "          node_dict[tail] = len(node_dict.keys())\n",
    "        if relation not in rel_dict:\n",
    "          rel_dict[relation] = len(rel_dict.keys())\n",
    "\n",
    "        edge_index.append([node_dict[head], node_dict[tail]])\n",
    "        edge_type.append(rel_dict[relation])\n",
    "\n",
    "      data_list.append(Data(x=x,\n",
    "                            edge_index=torch.tensor(edge_index).T,\n",
    "                            edge_type=torch.tensor(edge_type),\n",
    "                            y=int(targets[targets['id']==int(graph_id)]['label'])))\n",
    "\n",
    "    torch.save(self.collate(data_list), self.processed_paths[0])\n",
    "\n",
    "    torch.save(node_dict, self.processed_paths[1])\n",
    "    torch.save(rel_dict, self.processed_paths[2])\n",
    "\n",
    "\n",
    "def add_edges(dataset, reverse_edges=False, self_loops=False):\n",
    "\n",
    "  num_relations = dataset.num_relations\n",
    "\n",
    "  if reverse_edges:\n",
    "    dataset_new = []\n",
    "    for g in dataset:\n",
    "      row, col = g.edge_index\n",
    "      row, col = torch.cat([row, col], dim=0), torch.cat([col, row], dim=0)\n",
    "\n",
    "      g_new = Data(x=g.x,\n",
    "                    edge_index=torch.stack([row, col], dim=0),\n",
    "                    edge_type=torch.cat([g.edge_type , g.edge_type  + num_relations]),\n",
    "                    y=g.y)\n",
    "      dataset_new.append(g_new)\n",
    "    num_relations *= 2\n",
    "    dataset = dataset_new\n",
    "\n",
    "\n",
    "  if self_loops:\n",
    "    dataset_new = []\n",
    "    for g in dataset:\n",
    "      g_nodes = torch.unique(g.edge_index.flatten())\n",
    "\n",
    "      g_new = Data(x=g.x,\n",
    "                    edge_index=torch.cat((g.edge_index, torch.stack([g_nodes, g_nodes], dim=0)), dim=1),\n",
    "                    edge_type=torch.cat([g.edge_type , torch.full_like(g_nodes, num_relations)]),\n",
    "                    y=g.y)\n",
    "      dataset_new.append(g_new)\n",
    "    num_relations += 1\n",
    "\n",
    "  return dataset_new, num_relations"
   ],
   "metadata": {
    "id": "_t9LyHBZrchm"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = GPClassification('./data/GPClassificationTest_01/', dataset_name='GPClassificationTest_01')\n",
    "num_classes = dataset.num_classes\n",
    "x = dataset[0].x\n",
    "print(dataset.num_relations)\n",
    "dataset, num_relations = add_edges(dataset, reverse_edges=True, self_loops=True)\n",
    "torch.manual_seed(12345)\n",
    "random.shuffle(dataset)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "id": "jWufec55ssr_",
    "outputId": "4b1ed413-47c7-4dfb-9fed-9b1610677433"
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please place the required files in the raw directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/GPClassificationTest_01/raw/graphs.json'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mGPClassification\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m./data/GPClassificationTest_01/\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mGPClassificationTest_01\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m num_classes \u001B[38;5;241m=\u001B[39m dataset\u001B[38;5;241m.\u001B[39mnum_classes\n\u001B[1;32m      3\u001B[0m x \u001B[38;5;241m=\u001B[39m dataset[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mx\n",
      "Cell \u001B[0;32mIn[2], line 7\u001B[0m, in \u001B[0;36mGPClassification.__init__\u001B[0;34m(self, root, dataset_name, transform, pre_transform)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, root: \u001B[38;5;28mstr\u001B[39m, dataset_name: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGPClassificationTest_01\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      3\u001B[0m             transform: Optional[Callable] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m      4\u001B[0m             pre_transform: Optional[Callable] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m      6\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset_name \u001B[38;5;241m=\u001B[39m dataset_name\n\u001B[0;32m----> 7\u001B[0m   \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mroot\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransform\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpre_transform\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mslices \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocessed_paths[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m     10\u001B[0m   node_dict \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocessed_paths[\u001B[38;5;241m1\u001B[39m])\n",
      "File \u001B[0;32m~/miniconda3_m1/envs/cbilp/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:57\u001B[0m, in \u001B[0;36mInMemoryDataset.__init__\u001B[0;34m(self, root, transform, pre_transform, pre_filter, log)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     51\u001B[0m     root: Optional[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     55\u001B[0m     log: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     56\u001B[0m ):\n\u001B[0;32m---> 57\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mroot\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransform\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpre_transform\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpre_filter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlog\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     58\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     59\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mslices \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3_m1/envs/cbilp/lib/python3.9/site-packages/torch_geometric/data/dataset.py:97\u001B[0m, in \u001B[0;36mDataset.__init__\u001B[0;34m(self, root, transform, pre_transform, pre_filter, log)\u001B[0m\n\u001B[1;32m     94\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_download()\n\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhas_process:\n\u001B[0;32m---> 97\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_process\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3_m1/envs/cbilp/lib/python3.9/site-packages/torch_geometric/data/dataset.py:230\u001B[0m, in \u001B[0;36mDataset._process\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    227\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mProcessing...\u001B[39m\u001B[38;5;124m'\u001B[39m, file\u001B[38;5;241m=\u001B[39msys\u001B[38;5;241m.\u001B[39mstderr)\n\u001B[1;32m    229\u001B[0m makedirs(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocessed_dir)\n\u001B[0;32m--> 230\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    232\u001B[0m path \u001B[38;5;241m=\u001B[39m osp\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocessed_dir, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpre_transform.pt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    233\u001B[0m torch\u001B[38;5;241m.\u001B[39msave(_repr(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpre_transform), path)\n",
      "Cell \u001B[0;32mIn[2], line 31\u001B[0m, in \u001B[0;36mGPClassification.process\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprocess\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m     29\u001B[0m   data_list, node_dict, rel_dict \u001B[38;5;241m=\u001B[39m [], {}, {}\n\u001B[0;32m---> 31\u001B[0m   graph_dict \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mosp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraw_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mgraphs.json\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     33\u001B[0m   targets \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(osp\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw_dir, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtargets.csv\u001B[39m\u001B[38;5;124m'\u001B[39m), header\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,  names\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     35\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_nodes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(targets[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[0;32m~/miniconda3_m1/envs/cbilp/lib/python3.9/site-packages/IPython/core/interactiveshell.py:284\u001B[0m, in \u001B[0;36m_modified_open\u001B[0;34m(file, *args, **kwargs)\u001B[0m\n\u001B[1;32m    277\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[1;32m    278\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    279\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    280\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    281\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    282\u001B[0m     )\n\u001B[0;32m--> 284\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'data/GPClassificationTest_01/raw/graphs.json'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# todo implement balancing\n",
    "\n",
    "train, val, test = torch.utils.data.random_split(dataset, [0.7, 0.15, 0.15])"
   ],
   "metadata": {
    "id": "bWbP1St7vHUT"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from torch_geometric.loader import DataLoader, DataListLoader\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=64, shuffle=True)  # , sampler=sampler)\n",
    "val_loader = DataLoader(val, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test, batch_size=64, shuffle=False)\n",
    "\n",
    "for step, data in enumerate(train_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y8VR9R-htj9a",
    "outputId": "ba9fa7aa-e754-4eb5-c154-35816d7d21b9"
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[7808], edge_index=[2, 244], y=[64], edge_type=[244], batch=[7808], ptr=[65])\n",
      "Step 2:\n",
      "=======\n",
      "Number of graphs in the current batch: 20\n",
      "DataBatch(x=[2440], edge_index=[2, 77], y=[20], edge_type=[77], batch=[2440], ptr=[21])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([122])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, RGCNConv, GINEConv, MLP\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, layer_type, hidden_channels, num_nodes, trainable_embeddings=True):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "\n",
    "        self.layer_type = layer_type\n",
    "        print(num_nodes, 50)\n",
    "\n",
    "        self.emb = torch.nn.Embedding(num_nodes, 50, _freeze=trainable_embeddings)\n",
    "        self.lin = Linear(hidden_channels, num_classes)\n",
    "\n",
    "        if layer_type == 'GCN':\n",
    "            self.conv1 = GCNConv(50, hidden_channels)\n",
    "            self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "            self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "        elif layer_type == 'RGCN':\n",
    "            self.conv1 = RGCNConv(50, hidden_channels, num_relations=num_relations)\n",
    "            self.conv2 = RGCNConv(hidden_channels, hidden_channels, num_relations=num_relations)\n",
    "            self.conv3 = RGCNConv(hidden_channels, hidden_channels, num_relations=num_relations)\n",
    "\n",
    "        elif layer_type == 'GINE':\n",
    "            self.conv1 = GINEConv(MLP([50, hidden_channels]), edge_dim=num_relations)\n",
    "            self.conv2 = GINEConv(MLP([hidden_channels, hidden_channels]), edge_dim=num_relations)\n",
    "            self.conv3 = GINEConv(MLP([hidden_channels, hidden_channels]), edge_dim=num_relations)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_type, batch):\n",
    "\n",
    "        edge_features = None\n",
    "        if self.layer_type == 'RGCN':\n",
    "            edge_features = edge_type\n",
    "        elif self.layer_type == 'GINE':\n",
    "            edge_features = torch.nn.functional.one_hot(edge_type, num_classes=num_relations).to(torch.float)\n",
    "\n",
    "        # 1. Obtain node embeddings\n",
    "        x = self.conv1(self.emb.weight[x.long()], edge_index, edge_features)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index, edge_features)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index, edge_features)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return x\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q5PDgu8nCFQY",
    "outputId": "e9e9a015-d22d-4997-dd2a-b438819bde44"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = GCN(layer_type='RGCN', hidden_channels=64, num_nodes=x.size(0), trainable_embeddings=False)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "        out = model(data.x.long(), data.edge_index, data.edge_type, data.batch)  # Perform a single forward pass.\n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "\n",
    "     correct = 0\n",
    "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "         out = model(data.x, data.edge_index, data.edge_type, data.batch)\n",
    "         pred = out.argmax(dim=1)  # Use the class with the highest probability.\n",
    "\n",
    "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "for epoch in range(1, 171):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "xT65aef2vLPq",
    "outputId": "48e51f38-c608-4e40-b59a-fd0e01cc01a8"
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122 50\n",
      "Epoch: 001, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 002, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 003, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 004, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 005, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 006, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 007, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 008, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 009, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 010, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 011, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 012, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 013, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 014, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 015, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 016, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 017, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 018, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 019, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 020, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 021, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 022, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 023, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 024, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 025, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 026, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 027, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 028, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 029, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 030, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 031, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 032, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 033, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 034, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 035, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 036, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 037, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 038, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 039, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 040, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 041, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 042, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 043, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 044, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 045, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 046, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 047, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 048, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 049, Train Acc: 0.8214, Test Acc: 0.8889\n",
      "Epoch: 050, Train Acc: 0.8452, Test Acc: 0.8889\n",
      "Epoch: 051, Train Acc: 0.8571, Test Acc: 0.8889\n",
      "Epoch: 052, Train Acc: 0.9167, Test Acc: 0.8889\n",
      "Epoch: 053, Train Acc: 1.0000, Test Acc: 0.8889\n",
      "Epoch: 054, Train Acc: 1.0000, Test Acc: 0.8889\n",
      "Epoch: 055, Train Acc: 1.0000, Test Acc: 0.8889\n",
      "Epoch: 056, Train Acc: 1.0000, Test Acc: 0.8889\n",
      "Epoch: 057, Train Acc: 1.0000, Test Acc: 0.8889\n",
      "Epoch: 058, Train Acc: 1.0000, Test Acc: 0.8889\n",
      "Epoch: 059, Train Acc: 1.0000, Test Acc: 0.9444\n",
      "Epoch: 060, Train Acc: 1.0000, Test Acc: 0.9444\n",
      "Epoch: 061, Train Acc: 1.0000, Test Acc: 0.9444\n",
      "Epoch: 062, Train Acc: 1.0000, Test Acc: 0.9444\n",
      "Epoch: 063, Train Acc: 1.0000, Test Acc: 0.9444\n",
      "Epoch: 064, Train Acc: 1.0000, Test Acc: 0.9444\n",
      "Epoch: 065, Train Acc: 1.0000, Test Acc: 0.9444\n",
      "Epoch: 066, Train Acc: 1.0000, Test Acc: 0.9444\n",
      "Epoch: 067, Train Acc: 1.0000, Test Acc: 0.9444\n",
      "Epoch: 068, Train Acc: 1.0000, Test Acc: 0.9444\n",
      "Epoch: 069, Train Acc: 1.0000, Test Acc: 0.9444\n",
      "Epoch: 070, Train Acc: 1.0000, Test Acc: 0.9444\n",
      "Epoch: 071, Train Acc: 1.0000, Test Acc: 0.9444\n",
      "Epoch: 072, Train Acc: 1.0000, Test Acc: 0.9444\n",
      "Epoch: 073, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 074, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 075, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 076, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 077, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 078, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 079, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 080, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 081, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 082, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 083, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 084, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 085, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 086, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 087, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 088, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 089, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 090, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 091, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 092, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 093, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 094, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 095, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 096, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 097, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 098, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 099, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 100, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 101, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 102, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 103, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 104, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 105, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 106, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 107, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 108, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 109, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 110, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 111, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 112, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 113, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 114, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 115, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 116, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 117, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 118, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 119, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 120, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 121, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 122, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 123, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 124, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 125, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 126, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 127, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 128, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 129, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 130, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 131, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 132, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 133, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 134, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 135, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 136, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 137, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 138, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 139, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 140, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 141, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 142, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 143, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 144, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 145, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 146, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 147, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 148, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 149, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 150, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 151, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 152, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 153, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 154, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 155, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 156, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 157, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 158, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 159, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 160, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 161, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 162, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 163, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 164, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 165, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 166, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 167, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 168, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 169, Train Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 170, Train Acc: 1.0000, Test Acc: 1.0000\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "WbtGbpVnCatk",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f92cb4dd-96a4-4b34-aec3-d56c3cd133d0"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "m0ihqRFPToab",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
